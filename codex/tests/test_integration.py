"""
Integration tests for the Super-Codex-AI system
Tests complete workflows from document ingestion to scroll generation.
"""

import pytest
import asyncio
import tempfile
import json
import shutil
from pathlib import Path
from unittest.mock import Mock, AsyncMock, patch
from typing import Dict, List, Any
import warnings

# Import the modules to test
import sys
sys.path.insert(0, str(Path(__file__).parent.parent))

from engine.app import CodexEngine
from engine.config import CodexConfig
from engine.rag import CodexRAG
from engine.ingest import CodexIngest
from scrolls.capsule import CapsuleRegistry, ScrollGenerator
from scrolls.realtime import RealtimeScrollManager


class TestFullSystemIntegration:
    """Test complete system workflows"""
    
    @pytest.fixture
    async def temp_workspace(self):
        """Create temporary workspace with all necessary directories"""
        temp_dir = Path(tempfile.mkdtemp())
        
        # Create directory structure
        directories = [
            "data/corpus",
            "data/vectors", 
            "data/audit",
            "scrolls/templates",
            "scrolls/generated",
            "config"
        ]
        
        for dir_path in directories:
            (temp_dir / dir_path).mkdir(parents=True, exist_ok=True)
        
        # Create sample documents
        sample_docs = {
            "honor_system.md": """
# Honor System Documentation

The Super-Codex-AI honor system is a comprehensive framework for recognizing
exceptional contributions and maintaining high standards of excellence.

## Key Principles

1. **Merit-based Recognition**: Awards based on demonstrated excellence
2. **Transparency**: Clear criteria and open processes
3. **Community Impact**: Focus on contributions that benefit the collective

## Honor Levels

- **Bronze**: Initial recognition for promising contributions
- **Silver**: Sustained excellence and leadership
- **Gold**: Exceptional impact and innovation
""",
            "technical_specs.md": """
# Technical Specifications

This document outlines the technical architecture and implementation
details for the Super-Codex-AI system.

## System Architecture

- **RAG Engine**: Vector-based retrieval and generation
- **Capsule System**: Specialized knowledge containers
- **Real-time Processing**: WebSocket-based live updates

## API Endpoints

- `/api/query`: Process natural language queries
- `/api/scrolls/generate`: Generate ceremonial scrolls
- `/api/realtime/connect`: WebSocket connection for live updates
""",
            "governance.json": json.dumps({
                "document_type": "governance",
                "title": "Ceremonial Governance Protocols",
                "version": "1.0",
                "sections": {
                    "authority_levels": {
                        "public": "Basic access to public information",
                        "custodian": "Administrative privileges and oversight",
                        "council": "Full governance and ceremonial authority"
                    },
                    "ceremonial_requirements": {
                        "minimum_authority": "custodian",
                        "witness_count": 2,
                        "documentation_required": True
                    }
                }
            })
        }
        
        # Write sample documents
        corpus_dir = temp_dir / "data" / "corpus"
        for filename, content in sample_docs.items():
            (corpus_dir / filename).write_text(content)
        
        # Create simple scroll template
        template_content = """
# {{ scroll_title }}

**Query**: {{ query }}
**Generated**: {{ timestamp }}
**Capsule**: {{ capsule_name }}
**Authority**: {{ ceremonial_context.authority_level }}

## Response

{{ main_response }}

## Knowledge Sources

{% for source in sources %}
### {{ source.metadata.file_name or "Unknown Source" }}

{{ source.content[:200] }}...

{% endfor %}

---

*Generated by Super-Codex-AI â€¢ Scroll ID: {{ scroll_id }}*
        """.strip()
        
        template_dir = temp_dir / "scrolls" / "templates"
        (template_dir / "integration_test.jinja").write_text(template_content)
        
        yield temp_dir
        
        # Cleanup
        shutil.rmtree(temp_dir)
    
    @pytest.fixture
    def test_config(self, temp_workspace):
        """Create test configuration"""
        with warnings.catch_warnings():
            warnings.simplefilter("ignore", DeprecationWarning)
            
            config = CodexConfig(
                corpus_path=temp_workspace / "data" / "corpus",
                vector_store_path=temp_workspace / "data" / "vectors" / "store.pkl",
                audit_log_path=temp_workspace / "data" / "audit" / "audit.json",
                capsule_registry_path=temp_workspace / "config" / "capsules.json",
                scroll_templates_path=temp_workspace / "scrolls" / "templates",
                generated_scrolls_path=temp_workspace / "scrolls" / "generated",
                chunk_size=500,
                chunk_overlap=50,
                similarity_threshold=0.7,
                max_search_results=5
            )
        
        return config
    
    @pytest.mark.asyncio
    async def test_document_ingestion_workflow(self, test_config):
        """Test complete document ingestion workflow"""
        # Initialize systems
        rag_system = CodexRAG(test_config)
        await rag_system.initialize()
        
        ingest_system = CodexIngest(test_config, rag_system)
        
        # Test ingestion of directory
        result = await ingest_system.ingest_directory(test_config.corpus_path)
        
        assert result["success"] is True
        assert result["total_files"] == 3  # honor_system.md, technical_specs.md, governance.json
        assert result["successful"] == result["total_files"]
        assert result["failed"] == 0
        assert result["success_rate"] == 1.0
        
        # Verify documents were indexed
        assert len(rag_system.vector_store.documents) > 0
        
        # Test querying indexed content
        query_result = await rag_system.query(
            "What are the honor levels in the system?",
            {"document_type": "documentation"}
        )
        
        assert query_result["success"] is True
        assert len(query_result["sources"]) > 0
        assert any("bronze" in src["content"].lower() for src in query_result["sources"])
        
        await rag_system.cleanup()
    
    @pytest.mark.asyncio
    async def test_scroll_generation_workflow(self, test_config, temp_workspace):
        """Test complete scroll generation workflow"""
        # Initialize systems
        rag_system = CodexRAG(test_config)
        await rag_system.initialize()
        
        # Create capsule registry with test capsule
        registry = CapsuleRegistry(test_config)
        
        # Mock the initialization to avoid loading defaults
        with patch.object(registry, '_initialize_default_capsules'):
            pass
        
        from scrolls.capsule import CapsuleDefinition, CapsuleType, AccessLevel
        from engine.models.prompts import ScrollType, PromptManager
        
        test_capsule = CapsuleDefinition(
            capsule_id="test_integration",
            name="Integration Test Capsule",
            capsule_type=CapsuleType.SCHOLAR,
            scroll_type=ScrollType.GENERAL,
            access_level=AccessLevel.PUBLIC,
            template_path="integration_test.jinja",
            description="Test capsule for integration testing"
        )
        
        registry.register_capsule(test_capsule)
        
        # Initialize scroll generator
        prompt_manager = PromptManager(test_config)
        generator = ScrollGenerator(test_config, registry, prompt_manager)
        
        # Ingest some test data
        await rag_system.ingest(
            "The honor system recognizes exceptional contributions through Bronze, Silver, and Gold levels.",
            "governance",
            {"document_type": "governance", "source": "honor_system.md"}
        )
        
        # Query for relevant information
        rag_result = await rag_system.query(
            "Explain the honor levels",
            {"document_type": "governance"}
        )
        
        assert rag_result["success"] is True
        
        # Generate scroll using the query results
        from scrolls.capsule import CeremonialContext
        ceremonial_context = CeremonialContext(
            actor="Integration Test User",
            realm="TEST-001",
            authority_level="public"
        )
        
        scroll_result = await generator.generate_scroll(
            "test_integration",
            "Explain the honor levels",
            rag_result["sources"],
            rag_result,
            {"authority_level": "public"},
            ceremonial_context
        )
        
        assert scroll_result["success"] is True
        assert "scroll_content" in scroll_result
        assert "Explain the honor levels" in scroll_result["scroll_content"]
        assert "Bronze, Silver, and Gold" in scroll_result["scroll_content"]
        assert "Integration Test Capsule" in scroll_result["scroll_content"]
        
        await rag_system.cleanup()
    
    @pytest.mark.asyncio
    async def test_real_time_workflow(self, test_config):
        """Test real-time scroll generation workflow"""
        # Initialize systems
        rag_system = CodexRAG(test_config)
        await rag_system.initialize()
        
        # Add some test data
        await rag_system.ingest(
            "Technical specifications include RAG Engine, Capsule System, and Real-time Processing.",
            "technical",
            {"document_type": "technical", "source": "specs.md"}
        )
        
        # Initialize real-time manager
        realtime_manager = RealtimeScrollManager(test_config)
        
        # Mock WebSocket and event handling
        mock_websocket = AsyncMock()
        mock_websocket.send = AsyncMock()
        
        # Start a scroll session
        session_id = "test_session_001"
        request_data = {
            "query": "What are the system components?",
            "capsule_id": "default_scholar",
            "user_context": {"authority_level": "public"}
        }
        
        session = realtime_manager.start_scroll_session(session_id, request_data)
        
        assert session is not None
        assert session.session_id == session_id
        assert session.status == "pending"
        
        # Simulate session progression
        session.start()
        assert session.status == "processing"
        
        session.update_progress(25, "querying_rag")
        assert session.progress == 25
        assert session.current_stage == "querying_rag"
        
        session.update_progress(50, "generating_scroll")
        assert session.progress == 50
        
        # Complete session
        result = {
            "scroll_content": "Generated scroll about system components",
            "scroll_metadata": {"confidence": 85}
        }
        session.complete(result)
        
        assert session.status == "completed"
        assert session.progress == 100
        assert session.result == result
        
        await rag_system.cleanup()
    
    @pytest.mark.asyncio
    async def test_full_engine_workflow(self, test_config):
        """Test complete engine workflow via main API"""
        # Initialize the main engine
        engine = CodexEngine(test_config)
        
        # Mock initialization methods that might require external dependencies
        with patch.object(engine.audit_system, 'initialize', new_callable=AsyncMock), \
             patch.object(engine.replay_system, 'initialize', new_callable=AsyncMock), \
             patch.object(engine, '_setup_realtime_endpoints'), \
             patch.object(engine.rag_system, 'initialize', new_callable=AsyncMock):
            
            await engine.initialize()
        
        # Test basic query processing
        from engine.app import CodexQueryRequest
        
        # Mock RAG query to avoid initialization issues
        mock_rag_result = {
            "success": True,
            "sources": [
                {
                    "content": "Mock search result about honor system",
                    "metadata": {"file_name": "mock.md", "document_type": "test"}
                }
            ],
            "confidence_score": 85,
            "processing_time": 0.5
        }
        
        with patch.object(engine.rag_system, 'query', new_callable=AsyncMock) as mock_query:
            mock_query.return_value = mock_rag_result
            
            query_request = CodexQueryRequest(
                query="What is the honor system?",
                context={"document_type": "governance"},
                user_id="test_user"
            )
            
            response = await engine.process_query(query_request)
            
            assert response.success is True
            assert response.query == "What is the honor system?"
            assert len(response.sources) > 0
            assert response.confidence_score == 85
            assert response.processing_time == 0.5
            
            # Verify audit tracking was called
            mock_query.assert_called_once_with(
                "What is the honor system?",
                {"document_type": "governance"}
            )
        
        await engine.cleanup()
    
    @pytest.mark.asyncio
    async def test_error_handling_workflow(self, test_config):
        """Test error handling in various workflow scenarios"""
        # Initialize systems
        rag_system = CodexRAG(test_config)
        await rag_system.initialize()
        
        # Test query with no results
        result = await rag_system.query(
            "Completely unrelated query about cooking recipes",
            {}
        )
        
        assert result["success"] is True  # Should still succeed but with no/low confidence
        assert result["confidence_score"] == 0 or len(result["sources"]) == 0
        
        # Test invalid capsule access
        registry = CapsuleRegistry(test_config)
        
        # Mock access check failure
        with patch.object(registry, 'check_access') as mock_check:
            mock_check.return_value = (False, "Access denied - insufficient authority")
            
            from engine.models.prompts import PromptManager
            prompt_manager = PromptManager(test_config)
            generator = ScrollGenerator(test_config, registry, prompt_manager)
            
            result = await generator.generate_scroll(
                "restricted_capsule",
                "Test query",
                [],
                {},
                {"authority_level": "public"}
            )
            
            assert result["success"] is False
            assert "Access denied" in result["error"]
        
        # Test real-time session errors
        realtime_manager = RealtimeScrollManager(test_config)
        
        session = realtime_manager.start_scroll_session(
            "error_session",
            {"query": "Test", "capsule_id": "test"}
        )
        
        session.error_out("Simulated processing error")
        
        assert session.status == "error"
        assert session.error == "Simulated processing error"
        assert session.completed_at is not None
        
        await rag_system.cleanup()


class TestPerformanceIntegration:
    """Test system performance under various loads"""
    
    @pytest.fixture
    def performance_config(self, tmp_path):
        """Configuration optimized for performance testing"""
        return CodexConfig(
            corpus_path=tmp_path / "corpus",
            vector_store_path=tmp_path / "vectors.pkl",
            chunk_size=200,  # Smaller chunks for faster processing
            chunk_overlap=20,
            similarity_threshold=0.6,
            max_search_results=3
        )
    
    @pytest.mark.asyncio
    async def test_batch_ingestion_performance(self, performance_config, tmp_path):
        """Test performance with batch document ingestion"""
        # Create multiple test documents
        corpus_dir = tmp_path / "corpus"
        corpus_dir.mkdir(parents=True)
        
        # Generate 10 test documents
        for i in range(10):
            doc_content = f"""
# Document {i}

This is test document number {i} containing various information
about the Super-Codex-AI system, honor protocols, and technical specifications.

Content includes details about:
- System architecture (document {i})
- Processing workflows (version {i})
- Integration patterns (iteration {i})

Total words: {100 + i * 10} approximately.
            """.strip()
            
            (corpus_dir / f"doc_{i:02d}.md").write_text(doc_content)
        
        # Initialize systems
        rag_system = CodexRAG(performance_config)
        await rag_system.initialize()
        
        ingest_system = CodexIngest(performance_config, rag_system)
        
        # Measure ingestion time
        import time
        start_time = time.time()
        
        result = await ingest_system.ingest_directory(corpus_dir)
        
        end_time = time.time()
        processing_time = end_time - start_time
        
        assert result["success"] is True
        assert result["total_files"] == 10
        assert result["successful"] == 10
        assert processing_time < 30  # Should complete in under 30 seconds
        
        # Test query performance
        start_time = time.time()
        
        query_result = await rag_system.query(
            "system architecture and workflows",
            {"document_type": "technical"}
        )
        
        end_time = time.time()
        query_time = end_time - start_time
        
        assert query_result["success"] is True
        assert len(query_result["sources"]) > 0
        assert query_time < 5  # Query should complete in under 5 seconds
        
        await rag_system.cleanup()
    
    @pytest.mark.asyncio
    async def test_concurrent_query_performance(self, performance_config):
        """Test performance with concurrent queries"""
        # Initialize system
        rag_system = CodexRAG(performance_config)
        await rag_system.initialize()
        
        # Add some test data
        test_docs = [
            "Honor system documentation with bronze, silver, gold levels",
            "Technical specifications for RAG engine implementation",
            "Ceremonial protocols for governance and recognition"
        ]
        
        for i, doc in enumerate(test_docs):
            await rag_system.ingest(
                doc,
                "test",
                {"doc_id": f"test_{i}", "document_type": "test"}
            )
        
        # Create multiple concurrent queries
        queries = [
            "What are the honor levels?",
            "How does the RAG engine work?",
            "Explain ceremonial protocols",
            "System architecture overview",
            "Recognition and governance"
        ]
        
        # Execute queries concurrently
        import time
        start_time = time.time()
        
        tasks = [
            rag_system.query(query, {"document_type": "test"})
            for query in queries
        ]
        
        results = await asyncio.gather(*tasks)
        
        end_time = time.time()
        total_time = end_time - start_time
        
        # Verify all queries succeeded
        assert len(results) == 5
        assert all(result["success"] for result in results)
        
        # Concurrent execution should be faster than sequential
        assert total_time < 10  # Should complete in under 10 seconds
        
        await rag_system.cleanup()


class TestSystemResilience:
    """Test system resilience and recovery"""
    
    @pytest.mark.asyncio
    async def test_graceful_degradation(self, tmp_path):
        """Test system behavior under adverse conditions"""
        config = CodexConfig(
            corpus_path=tmp_path / "corpus",
            vector_store_path=tmp_path / "vectors.pkl"
        )
        
        # Test with empty corpus
        rag_system = CodexRAG(config)
        await rag_system.initialize()
        
        # Query empty system
        result = await rag_system.query("Any query", {})
        
        assert result["success"] is True
        assert result["confidence_score"] == 0
        assert len(result["sources"]) == 0
        
        await rag_system.cleanup()
    
    @pytest.mark.asyncio
    async def test_cleanup_and_restart(self, tmp_path):
        """Test system cleanup and restart capabilities"""
        config = CodexConfig(
            corpus_path=tmp_path / "corpus",
            vector_store_path=tmp_path / "vectors.pkl"
        )
        
        # First initialization
        rag_system = CodexRAG(config)
        await rag_system.initialize()
        
        # Add some data
        await rag_system.ingest(
            "Test document for persistence",
            "test",
            {"test": True}
        )
        
        assert len(rag_system.vector_store.documents) == 1
        
        # Save and cleanup
        rag_system.vector_store.save()
        await rag_system.cleanup()
        
        # Restart system
        new_rag_system = CodexRAG(config)
        await new_rag_system.initialize()
        
        # Data should persist
        assert len(new_rag_system.vector_store.documents) == 1
        assert "Test document for persistence" in new_rag_system.vector_store.documents[0]
        
        await new_rag_system.cleanup()


if __name__ == "__main__":
    # Run integration tests with pytest
    pytest.main([__file__, "-v", "-s"])